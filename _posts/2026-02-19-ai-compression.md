---
title: "The Compression of Everything"
date: 2026-02-19
layout: post
---

# The Compression of Everything

If you re-encode a JPEG a hundred times, you don't get a blurry photo. You get fractal noise. The structure dissolves. Not into nothing — into a kind of everything-soup where no detail survives long enough to matter.

I keep thinking about this when I watch what's happening with LLMs.

## The default thing

More and more people are using Claude, ChatGPT, Gemini, Grok — not just for code or copywriting but for *decisions*. Business strategy. Life direction. Policy positions. And the outputs of these models, because of how they're built, converge.

This isn't a bug. It's the architecture. RLHF — reinforcement learning from human feedback — trains models to produce outputs that satisfy a narrow band of human raters. Fine-tuning collapses the rich, weird, sometimes-brilliant distribution of a base model into a handful of safe, palatable modes. The technical term is mode collapse. The plain-language version: the model learns to always give you the default thing.

And now millions of people are using the default thing to make their choices.

## Variance was doing more work than we thought

Here's the counterargument I keep hearing: people were already lazy, already taking shortcuts, already making shallow decisions. Cable news. Management consulting frameworks. The guy who runs his business on whatever book he read last quarter. LLMs just raise the floor.

Fine. But there's something those shortcuts had going for them: they were *different* shortcuts. One founder reads Taleb, another asks his uncle, another just follows her gut after a bad quarter. The outcomes are noisy. But the noise is *generative*. A thousand people making a thousand different mistakes will, collectively, explore much more of the solution space than a thousand people all asking the same model the same question and getting the same three answers in different wrappers.

When everyone routes through the same few models, trained on the same internet, optimised for the same reward signal — the shortcuts become correlated. That's not more of the same. That's a qualitative shift.

## Everyone can bullshit now

Society has always had rough filters for incompetence. Not perfect ones — plenty of frauds make it through, especially when institutions are broken. But generally, if someone is out of their depth, you can *tell*. The reasoning is thin. The memo rambles. The presentation doesn't survive a single hard question.

LLMs destroy this signal.

A mediocre city council member who would previously have submitted an obviously half-baked policy memo can now submit something that reads like it came from McKinsey. The person who spent three months studying water infrastructure and the person who spent three minutes prompting Claude about it now produce documents of roughly equal polish.

This breaks feedback loops that societies depend on. We allocate trust and authority partly based on demonstrated competence, and demonstrated competence used to require *actual* competence — or at least a significant investment in faking it, which itself required some understanding. That filter is gone. The appearance of competence has been decoupled from understanding, and it happened too fast for our institutions to adapt.

## The damage is at the meta-level

The deepest problem isn't that individual decisions get worse. Some will get better — the floor really does rise. The problem is that we lose the ability to *evaluate* decisions and decision-makers.

A society that can't distinguish the person who actually understands something from the person ventriloquising an LLM has lost a critical self-correction mechanism. And unlike most institutional failures, this one is invisible until it's too late. There's no early warning. The memos still look good. The strategy decks are still polished. The policy proposals still have the right structure and citations. Everything *looks* fine — right up until it isn't, and then the post-mortem is impossible because "the AI said to" isn't a reason anyone can learn from.

The errors don't metabolise into institutional knowledge. They just happen, and then they happen again.

## You've already seen the proof

Anyone still on LinkedIn can see the compression in real time. The "I'm thrilled to announce" cadence, the emoji bullets, the three-paragraph arc from humble beginning to lesson learned to inspirational takeaway. Nobody chose that format. It emerged from everyone routing through the same optimisation target, and now it reads like one voice through a thousand masks.

Image generators are the visual version. One Midjourney image is stunning. A feed full of them is the same artist with amnesia. Vibe-coded UIs are the interactive version — they look nice, they work, and if you've seen ten of them you've seen all of them. Same rounded corners, same Tailwind palette, same card-based layout, same subtle gradients.

The tell is never in any single output. It's in the aggregate. And a society is *always* the aggregate view.

## Progress comes from the tails

Here's what I keep coming back to. Raising the floor while lowering the ceiling *and removing the incentive to reach for the ceiling* is a net negative. Not because the average outcome gets worse — it might not. But because progress doesn't come from the average. It comes from the tails. From the person who spends an unreasonable amount of time thinking about a problem everyone else considers solved. From the strategy that sounds wrong to most people but turns out to be right. From the founder who ignores every standard playbook because she sees something nobody else does.

If we optimise for the median — and that is exactly what RLHF does, by construction — we get a society that's more competent on average and less capable of breakthroughs. Smooth, polished, and sterile. Like a JPEG that's been re-encoded so many times it's lost all the edges.

The structure dissolves. Not into nothing. Into everything-soup.

---

*I used an LLM to sharpen this argument. The irony is not lost on me — but at least I came with the raw material and knew what I was looking for. That distinction might matter more than we think.*
